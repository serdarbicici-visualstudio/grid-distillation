{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Distillation Different Dataset Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import grid_distillation as gd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class GridImageClusterSampler(ImageClusterSampler):\\n    def plot_clusters_with_grids(self, n_samples):\\n        plt.figure(figsize=(10, 6))\\n        for i in range(self.n_clusters):\\n            cluster_data = self.X_pca[self.cluster_labels == i]\\n            plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f'Cluster {i}', alpha=0.5)\\n        \\n        grid_size = int(np.ceil(np.sqrt(n_samples)))\\n        x_min, x_max = np.min(self.X_pca[:, 0]), np.max(self.X_pca[:, 0])\\n        y_min, y_max = np.min(self.X_pca[:, 1]), np.max(self.X_pca[:, 1])\\n        \\n        x_grid = np.linspace(x_min, x_max, grid_size + 1)\\n        y_grid = np.linspace(y_min, y_max, grid_size + 1)\\n        \\n        for x in x_grid:\\n            plt.axvline(x=x, color='k', linestyle='--', alpha=0.5)\\n        for y in y_grid:\\n            plt.axhline(y=y, color='k', linestyle='--', alpha=0.5)\\n        \\n        plt.title('Clustering of Cats and Dogs with Grids')\\n        plt.xlabel('PCA Component 1')\\n        plt.ylabel('PCA Component 2')\\n        plt.legend()\\n        plt.show()\\n    \\n    def get_grid_sampled_indices(self, cluster_indices, n_samples):\\n        if len(cluster_indices) <= n_samples:\\n            return cluster_indices\\n        \\n        grid_size = int(np.ceil(np.sqrt(n_samples)))\\n        x_min, x_max = np.min(self.X_pca[:, 0]), np.max(self.X_pca[:, 0])\\n        y_min, y_max = np.min(self.X_pca[:, 1]), np.max(self.X_pca[:, 1])\\n        \\n        x_grid = np.linspace(x_min, x_max, grid_size + 1)\\n        y_grid = np.linspace(y_min, y_max, grid_size + 1)\\n        \\n        sampled_indices = []\\n        for i in range(grid_size):\\n            for j in range(grid_size):\\n                x_start, x_end = x_grid[i], x_grid[i + 1]\\n                y_start, y_end = y_grid[j], y_grid[j + 1]\\n                \\n                grid_indices = [\\n                    idx for idx in cluster_indices \\n                    if x_start <= self.X_pca[idx, 0] < x_end and y_start <= self.X_pca[idx, 1] < y_end\\n                ]\\n                \\n                if grid_indices:\\n                    sampled_indices.append(np.random.choice(grid_indices))\\n\\n        while len(sampled_indices) < n_samples:\\n            sampled_indices.append(np.random.choice(cluster_indices))\\n        \\n        return sampled_indices[:n_samples]\\n\\n    def get_selected_samples(self, n_samples):\\n        selected_samples = []\\n        for label in range(2):\\n            label_indices = np.where(self.y == label)[0]\\n            label_cluster_labels = self.cluster_labels[label_indices]\\n            unique_clusters = np.unique(label_cluster_labels)\\n            \\n            for cluster_id in unique_clusters:\\n                cluster_indices = label_indices[label_cluster_labels == cluster_id]\\n                selected_indices = self.get_grid_sampled_indices(cluster_indices, n_samples)\\n                selected_samples.extend(selected_indices)\\n        return selected_samples\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class GridImageClusterSampler(ImageClusterSampler):\n",
    "    def plot_clusters_with_grids(self, n_samples):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_data = self.X_pca[self.cluster_labels == i]\n",
    "            plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f'Cluster {i}', alpha=0.5)\n",
    "        \n",
    "        grid_size = int(np.ceil(np.sqrt(n_samples)))\n",
    "        x_min, x_max = np.min(self.X_pca[:, 0]), np.max(self.X_pca[:, 0])\n",
    "        y_min, y_max = np.min(self.X_pca[:, 1]), np.max(self.X_pca[:, 1])\n",
    "        \n",
    "        x_grid = np.linspace(x_min, x_max, grid_size + 1)\n",
    "        y_grid = np.linspace(y_min, y_max, grid_size + 1)\n",
    "        \n",
    "        for x in x_grid:\n",
    "            plt.axvline(x=x, color='k', linestyle='--', alpha=0.5)\n",
    "        for y in y_grid:\n",
    "            plt.axhline(y=y, color='k', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.title('Clustering of Cats and Dogs with Grids')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_grid_sampled_indices(self, cluster_indices, n_samples):\n",
    "        if len(cluster_indices) <= n_samples:\n",
    "            return cluster_indices\n",
    "        \n",
    "        grid_size = int(np.ceil(np.sqrt(n_samples)))\n",
    "        x_min, x_max = np.min(self.X_pca[:, 0]), np.max(self.X_pca[:, 0])\n",
    "        y_min, y_max = np.min(self.X_pca[:, 1]), np.max(self.X_pca[:, 1])\n",
    "        \n",
    "        x_grid = np.linspace(x_min, x_max, grid_size + 1)\n",
    "        y_grid = np.linspace(y_min, y_max, grid_size + 1)\n",
    "        \n",
    "        sampled_indices = []\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                x_start, x_end = x_grid[i], x_grid[i + 1]\n",
    "                y_start, y_end = y_grid[j], y_grid[j + 1]\n",
    "                \n",
    "                grid_indices = [\n",
    "                    idx for idx in cluster_indices \n",
    "                    if x_start <= self.X_pca[idx, 0] < x_end and y_start <= self.X_pca[idx, 1] < y_end\n",
    "                ]\n",
    "                \n",
    "                if grid_indices:\n",
    "                    sampled_indices.append(np.random.choice(grid_indices))\n",
    "\n",
    "        while len(sampled_indices) < n_samples:\n",
    "            sampled_indices.append(np.random.choice(cluster_indices))\n",
    "        \n",
    "        return sampled_indices[:n_samples]\n",
    "\n",
    "    def get_selected_samples(self, n_samples):\n",
    "        selected_samples = []\n",
    "        for label in range(2):\n",
    "            label_indices = np.where(self.y == label)[0]\n",
    "            label_cluster_labels = self.cluster_labels[label_indices]\n",
    "            unique_clusters = np.unique(label_cluster_labels)\n",
    "            \n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_indices = label_indices[label_cluster_labels == cluster_id]\n",
    "                selected_indices = self.get_grid_sampled_indices(cluster_indices, n_samples)\n",
    "                selected_samples.extend(selected_indices)\n",
    "        return selected_samples\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3069718890719946\n",
      "Epoch [2/10], Loss: 2.303570016225179\n",
      "Epoch [3/10], Loss: 2.3020217312706843\n",
      "Epoch [4/10], Loss: 2.2982140329149034\n",
      "Epoch [5/10], Loss: 2.2692815992567272\n",
      "Epoch [6/10], Loss: 2.102637102868822\n",
      "Epoch [7/10], Loss: 1.7324073049757216\n",
      "Epoch [8/10], Loss: 1.4329083972507053\n",
      "Epoch [9/10], Loss: 1.2389584064483643\n",
      "Epoch [10/10], Loss: 1.0651254759894477\n",
      "Original Model Accuracy: 74.72222222222223%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     91\u001b[0m grid \u001b[38;5;241m=\u001b[39m gd\u001b[38;5;241m.\u001b[39mGridImageClusterSampler(X_train, y_train, n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_clusters_with_grids\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# get the selected samples\u001b[39;00m\n\u001b[0;32m     95\u001b[0m selected_samples \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mget_selected_samples(\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\summer_projects\\grid-distillation\\grid_distillation.py:109\u001b[0m, in \u001b[0;36mGridImageClusterSampler.plot_clusters_with_grids\u001b[1;34m(self, n_samples)\u001b[0m\n\u001b[0;32m    107\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters):\n\u001b[1;32m--> 109\u001b[0m     cluster_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_pca\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    110\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(cluster_data[:, \u001b[38;5;241m0\u001b[39m], cluster_data[:, \u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    112\u001b[0m grid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(np\u001b[38;5;241m.\u001b[39msqrt(n_samples)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.images\n",
    "y = digits.target\n",
    "\n",
    "# Normalize the images\n",
    "X = X / 255.0\n",
    "\n",
    "# Reshape the images to (n_samples, 1, 8, 8)\n",
    "X = X.reshape(-1, 1, 8, 8)\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), y_train)\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 2 * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 2 * 2)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Train and evaluate the original model\n",
    "train(model, train_loader, criterion, optimizer, epochs=10)\n",
    "original_accuracy = evaluate(model, test_loader)\n",
    "print(f'Original Model Accuracy: {original_accuracy}%')\n",
    "\n",
    "# distill the dataset by grid\n",
    "n_samples = 100\n",
    "grid = gd.GridImageClusterSampler(X_train, y_train, n_clusters=10)\n",
    "grid.plot_clusters_with_grids(100)\n",
    "\n",
    "# get the selected samples\n",
    "selected_samples = grid.get_selected_samples(100)\n",
    "\n",
    "# get the distilled dataset\n",
    "X_distilled = X_train[selected_samples]\n",
    "y_distilled = y_train[selected_samples]\n",
    "# Convert distilled dataset to torch tensors\n",
    "X_distilled = torch.tensor(X_distilled, dtype=torch.float32)\n",
    "y_distilled = torch.tensor(y_distilled, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader for the distilled dataset\n",
    "distilled_dataset = TensorDataset(X_distilled, y_distilled)\n",
    "distilled_loader = DataLoader(distilled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize a new model for the distilled dataset\n",
    "model_distilled = SimpleCNN()\n",
    "optimizer_distilled = optim.Adam(model_distilled.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate the distilled model\n",
    "train(model_distilled, distilled_loader, criterion, optimizer_distilled, epochs=10)\n",
    "distilled_accuracy = evaluate(model_distilled, test_loader)\n",
    "print(f'Distilled Model Accuracy: {distilled_accuracy}%')\n",
    "\n",
    "# Compare the performance of the two models\n",
    "print(f'Original Model Accuracy: {original_accuracy}%')\n",
    "print(f'Distilled Model Accuracy: {distilled_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# create a model in pytorch\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# load a dataset(sklearn laod digits) of images and labels from a directory, \n",
    "# train a pytorch model on it, distill the dataset (grid) and train a new model on the distilled dataset\n",
    "# and compare the performance of the two models\n",
    "\n",
    "# load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.images\n",
    "\n",
    "# reshape the images to 3D\n",
    "X = X.reshape(-1, 8, 8, 1)\n",
    "\n",
    "# normalize the images\n",
    "X = X / 255\n",
    "\n",
    "# one hot encode the labels\n",
    "y = digits.target\n",
    "y = np.eye(10)[y]\n",
    "\n",
    "# split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a model in torch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# distill the dataset\n",
    "# create a grid\n",
    "grid = gd.GridImageClusterSampler(X_train, y_train, n_clusters=10, n_samples=10)\n",
    "grid.plot_clusters_with_grids(10)\n",
    "\n",
    "# get the selected samples\n",
    "selected_samples = grid.get_selected_samples(100)\n",
    "\n",
    "# get the distilled dataset\n",
    "X_distilled = X_train[selected_samples]\n",
    "y_distilled = y_train[selected_samples]\n",
    "\n",
    "# use the same model architecture\n",
    "model_distilled = Sequential()\n",
    "model_distilled.add(Conv2D(32, (3, 3), activation='relu', input_shape=(8, 8, 1)))\n",
    "model_distilled.add(MaxPooling2D((2, 2)))\n",
    "model_distilled.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_distilled.add(MaxPooling2D((2, 2)))\n",
    "model_distilled.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_distilled.add(Flatten())\n",
    "model_distilled.add(Dense(64, activation='relu'))\n",
    "model_distilled.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_distilled.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the distilled model\n",
    "model_distilled.fit(X_distilled, y_distilled, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# compare the performance of the two models\n",
    "model.evaluate(X_test, y_test)\n",
    "model_distilled.evaluate(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_deneme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
